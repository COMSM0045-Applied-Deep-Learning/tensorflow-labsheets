{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Improving your training through batches, learning-rate and initialisation\n",
    "\n",
    "\n",
    "## 1. Setup (Copy Lab 2 Code)\n",
    "\n",
    "1. Login to BC4\n",
    "\n",
    "    ```\n",
    "    ssh <your_UoB_ID>@bc4login.acrc.bris.ac.uk\n",
    "    ```\n",
    "    \n",
    "2. Clone the repository\n",
    "\n",
    "    ```\n",
    "    git clone \"https://github.com/COMSM0018-Applied-Deep-Learning/labsheets.git\" ~/labsheets\n",
    "    ```\n",
    "\n",
    "3. Change to the lab 3 directory:\n",
    "\n",
    "    ```\n",
    "    cd ~/labsheets/Lab_3_Training/\n",
    "    ```\n",
    "    \n",
    "4. Make all ```go_interactive.sh``` and ```tensorboard_params.sh``` files executables by using the command `chmod`: \n",
    "\n",
    "    ```\n",
    "    chmod +x go_interactive.sh tensorboard_params.sh\n",
    "    ```\n",
    "   \n",
    "5. Switch to interactive mode, and note the change of the gpu login to a reserved gpu:\n",
    "\n",
    "    ```\n",
    "    ./go_interactive.sh \n",
    "    ```\n",
    "    \n",
    "6. Run the following script. It will pop up two values: `ipnport=XXXXX` and `ipnip=XX.XXX.X.X.`\n",
    "\n",
    "    ```\n",
    "    ./tensorboard_params.sh\n",
    "    ```\n",
    "    \n",
    "    **Write them down since we will use them for using TensorBoard.**\n",
    "\n",
    "7. Copy your code from Lab 2 - and **rename the file to ``cifar_hyperparam.py``**. Run to check for correctness:\n",
    "    \n",
    "    ```\n",
    "    python cifar_hyperparam.py\n",
    "    ```\n",
    "   \n",
    "8. Open a **new terminal window** and login using SSH like in step 1 then run:\n",
    "\n",
    "    ```\n",
    "    tensorboard --logdir=logs/ --port=<ipnport>\n",
    "    ```\n",
    "    \n",
    "9. Open a **new terminal window** on your machine and type: \n",
    "    \n",
    "    ```\n",
    "    ssh -N <USER_NAME>@bc4login.acrc.bris.ac.uk -L 6006:<ipnip>:<ipnport>\n",
    "    ```\n",
    "\n",
    "10. Open your web browser (Use Chrome; Firefox currently has issues with tensorboard) and open the port 6006 (http://localhost:6006). This should open TensorBoard, and you can navigate through the summaries that we included.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch-based training\n",
    "\n",
    "In the first part of this lab session we shall see what batch-based training means.  \n",
    "\n",
    "### What is a batch?\n",
    "\n",
    "They say there is no such thing as a stupid question, so let's first make sure we all know what a batch is.  \n",
    "**A batch is a set of data points**.  \n",
    "For example, in CIFAR-10, a data point is a single image, while a batch of size 16 is a set of 16 images. \n",
    "\n",
    "#### What is a training batch?\n",
    "\n",
    "It is a set of data points we use to train our network.\n",
    "\n",
    "#### What is batch-based training?\n",
    "\n",
    "Batch-based training refers to train a neural network using multiple data points as opposed to a single data point. We referred to the gradient descent optimisation that uses a batch as *stochastic gradient descent*.\n",
    "As a matter of fact, there is no real network that can learn anything sensible without batch training.\n",
    "\n",
    "### Hang on a minute, didn't our CNN take a single image as input?\n",
    "\n",
    "Yes, it does. Our first CNN takes one single image as input and outputs the confidence of the network of recognising that image as one of the 10 objects.  \n",
    "However, when we train our network, we do it considering a batch of images, i.e. we adjust the parameters of the network by looking at multiple images, as opposed to just one.\n",
    "\n",
    "### Why is batch-based training important?\n",
    "\n",
    "For example, in CIFAR-10, a data point is a single image, while a batch of size 16 is a set of 16 images. \n",
    "\n",
    "### Let's have a look\n",
    "\n",
    "Let's have a look now at the code in ``cifer_hyperparm.py``, which you copied from ``Lab_2``. Amongst the parameters (flags) we set, notice now the following:\n",
    "\n",
    "```python\n",
    "tf.app.flags.DEFINE_integer('batch-size', 128)\n",
    "```\n",
    "As you'll have already guessed, this parameter controls the batch size, i.e. how many images will be used in a mini-batch to train the network.  \n",
    "Let's see how much this parameter impacts the accuracy of our model.  \n",
    "\n",
    "### PRACTICAL 2.1: Adjusting the batch size\n",
    "\n",
    "1. Change the learning rate to 1.00E-03\n",
    "```python\n",
    "tf.app.flags.DEFINE_float('learning-rate', 1e-3)\n",
    "```\n",
    "2. Train your model **as is** using the current batch size and training rate, and note the **test accuracy**\n",
    "3. Set the batch size to 16, and train your model. What is the effect on the **test accuracy**?\n",
    "4. Now train with a batch size of 256\n",
    "5. Save your logs and training rate (only for your 256 batch size run)\n",
    "\n",
    "```\n",
    "Lab_3_<username>.zip\n",
    " |----------logs\\ \n",
    "            |----------exp_bs_256_lr_0.001_train\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    "            |----------exp_bs_256_lr_0.001_validate\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    " |----------run_exp_bs_256_lr_0.001_train,tag_Loss.csv\n",
    " ```\n",
    "\n",
    "Discuss with others in the lab what happens to the **test accuracy**\n",
    "\n",
    "### To conclude\n",
    "\n",
    "The following table reports the test accuracy obtained training the very same model changing the batch size. You should get similar results.\n",
    "\n",
    "Batch size | Test accuracy  \n",
    "---------- | -------------------  \n",
    "1|0.1749\n",
    "2|0.1796\n",
    "4|0.1934\n",
    "8|0.2171\n",
    "16|0.24\n",
    "32|0.2427\n",
    "64|0.2585\n",
    "128|0.2886\n",
    "256|0.3015\n",
    "512|0.3194\n",
    "1024|0.314\n",
    "2048|0.3283\n",
    "**4096**|**0.3474**\n",
    "\n",
    "![image.png](imgs/accuracyVsBatchSize.png)\n",
    "\n",
    "What can we deduce from the above results?\n",
    "\n",
    "It seems that the larger the mini-batch the better, right?  \n",
    "\n",
    "**Q. What stops us using the whole training set in every training step?**\n",
    "**Q. Why do people typically use batch sizes of 32, 64 or 128?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning rate\n",
    "\n",
    "The learning rate is a hyperparameter that controls how fast we descend the gradient while we train our models. Recall from our lectures the following:  \n",
    "\n",
    "$$W_{t+1} = W_t - \\eta \\nabla J(\\textbf{x}, W_t)$$\n",
    "\n",
    "Where:  \n",
    "\n",
    "* $W_{t+1}$ are the updated model parameters\n",
    "* $W_t$ are the model parameters at the previous step\n",
    "* $\\eta$ **is the learning rate**\n",
    "* $\\textbf{x}$ is the input data to the model\n",
    "* $\\nabla J$ is the gradient of the loss function $J$\n",
    "\n",
    "Depending mainly on the input data and the loss function, the learning rate may have a very strong impact on the overall performance of the network.  \n",
    "\n",
    "In our code, this parameter is controlled by the following flag:  \n",
    "\n",
    "```python\n",
    "tf.app.flags.DEFINE_float('learning-rate', 1e-3)\n",
    "```\n",
    "\n",
    "Let's fix the batch size **BACK** to 128 (a good compromise between speed and accuracy) and let's try to run the same model for the same number of steps (10000) with different learning rates.\n",
    "\n",
    "### PRACTICAL 2.2: Adjusting the learning rate\n",
    "\n",
    "1. Set the batch size back to 128 \n",
    "2. Change learning rate to 1.00E-04\n",
    "3. Train your model and save the relevant logs\n",
    "\n",
    "```\n",
    "Lab_3_<username>.zip\n",
    " |----------logs\\ \n",
    "            |----------exp_bs_128_lr_0.0001_train\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    "            |----------exp_bs_128_lr_0.0001_validate\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    " |----------run_exp_bs_128_lr_0.0001_train,tag_Loss.csv\n",
    " ```\n",
    "\n",
    "\n",
    "### To Conclude\n",
    "\n",
    "Learning rate | Test accuracy  \n",
    "---------- | -------------------  \n",
    "1.00E-01|0.1\n",
    "1.00E-02|0.1\n",
    "1.00E-03|0.251\n",
    "**1.00E-04**|**0.4977**\n",
    "1.00E-05|0.4198\n",
    "1.00E-06|0.2998\n",
    "1.00E-07|0.1338\n",
    "\n",
    "![image.png](imgs/accuracyVsLearningRate.png)\n",
    "\n",
    "Did you notice that with a learning rate of 1.00E-04 we managed to reach 49% accuracy?  \n",
    "Hang on a second, wasn't 35% our best shot with batch size equal to 4096? Well, yes, but in fact in the previous experiments the learning rate was fixed to 1.00E-03.\n",
    "\n",
    "Let's now have a look at the loss graphs provided by Tensorboard  \n",
    "\n",
    "![lossGraphDifferentLearningRates.png](imgs/lossGraphDifferentLearningRates.png)\n",
    "\n",
    "The above graph plots the evolution of the loss function over time using the different learning rates we saw before.\n",
    "\n",
    "\n",
    "**Q: Try to explain why we experience the results and the graphs above.**\n",
    "\n",
    "**Q: What do you think would happen if we train the model with a fixed small learning rate (e.g. 1.00E-05) and double the number of steps? **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decaying the learning rate \n",
    "\n",
    "In practice, many gradient descent algorithms work with a non constant learning rate, that is the learning rate is lowered over time during training.  \n",
    "The basic concept behind learning rate decay is that we first descend the gradient more quickly when we start training the model, say when we \"still have much to learn\", and then slow down as we proceed, since learning \"new things\" after a while requires more time and care.  \n",
    "\n",
    "We are using Adam optimiser as our gradient-based optimisation algorithm, which typically uses learning rate decay. However, up until now, we have trained our model using the fixed rate defined by `FLAGS.learning_rate`:\n",
    "\n",
    "```python\n",
    "train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)\n",
    "```\n",
    "\n",
    "It's quite simple to change our code so that we decay the learning rate while training. A commonly used way of lowering the learning rate is to apply an exponential decay as follows:  \n",
    "\n",
    "$$ decayed\\_learning\\_rate = learning\\_rate \\cdot {decay\\_rate}^{(global\\_step \\ / \\ decay\\_steps)} $$\n",
    "\n",
    "There is a function in tensorflow to compute the decayed learning rate as defined above:\n",
    "\n",
    "```python\n",
    "tf.train.exponential_decay(\n",
    "    learning_rate,\n",
    "    global_step,\n",
    "    decay_steps,\n",
    "    decay_rate\n",
    ")\n",
    "```\n",
    "\n",
    "### PRACTICAL 2.3: Learning rate decay in `cifar_hyperparam.py`\n",
    "\n",
    "\n",
    "1. Find the line in your code that sets the learning rate \n",
    "\n",
    "    ```python\n",
    "    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)\n",
    "    ```\n",
    "    \n",
    "2. **NOW** Comment this line out, you need to define a decayling learning rate using [``tf.train.exponential_decay``](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay). This will give you a decaying variable. Use 1000 decay steps and 0.8 decay rate. \n",
    "\n",
    "3. **NOW** pass your decaying learning rate variable to a new AdamOptimiser, that minimises cross_entropy.\n",
    "\n",
    "**Run the model** (you do not need to save any logs at this time)\n",
    "\n",
    "We can observe a small boost in accuracy\n",
    "\n",
    "Let's compare the two loss functions (blue is with learning rate decay, orange is without):\n",
    "\n",
    "![lossGraphLearningRateDecay.png](imgs/lossGraphLearningRateDecay.png)\n",
    "\n",
    "  \n",
    "* **Q: What can we deduce from the above graphs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch normalisation\n",
    "\n",
    "The distribution of each layer's input, i.e. either the input data itself or the output from intermediate layers, plays a crucial role when training (especially) deep neural networks. The main issue is that the distribution of the input data (recall we are using batch-based training) - and thus the distibution of the input to the following layers - varies while we train and while we learn the optimal parameters for our task. This phenomenon is typically referred to as _internal covariate shift_.\n",
    "\n",
    "It would be helpful for our network if such distributions didn't change much as we observe new input data and generate intermediate layer outputs while training and adjusting the model's parameters.\n",
    "\n",
    "This is what batch normalisation does by normalising layer inputs. It was proposed by Sergey Ioffe and Christian Szegedy at Google [in this paper](https://arxiv.org/pdf/1502.03167.pdf). Let's see in detail how it works. \n",
    "\n",
    "From the paper:  \n",
    "\n",
    ">By fixing the distribution of the layer inputs as the training progresses, we expect to improve the training speed. It has been long known that the network training converges faster if its inputs are whitened â€“ i.e., linearly transformed to have zero means and unit variances, and decorrelated. As each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer.  \n",
    "\n",
    "Let $\\text{x} = (\\textit{x}^{(1)}, \\dots, \\textit{x}^{(d)})$ be a d-dimensional input vector to a certain layer in our network. We have to normalise each scalar feature independently by making it have mean 0 and variance 1. We do this for each dimension $k$:\n",
    "\n",
    "$$\n",
    "\\bar{x}^{(k)} = \\frac{x^{(k)} - \\text{E}[x^{(k)}]}{\\sqrt{\\text{Var}[x^{(k)}] + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Where the expectation and variance are computed **over the training batch**, and $\\epsilon$ is a constant added for numerical stability. Let's quote again the paper now:\n",
    "\n",
    ">Such normalization speeds up convergence, even when the features are not decorrelated. Note that simply normalizing each input of a layer may change what the layer can represent. To address this, we make sure that the transformation inserted in the network can represent the identity transform.\n",
    "\n",
    "To do so, we add for each dimension $k$ a pair of new parameters $\\gamma ^{(k)}, \\beta ^{(k)}$. The output of the layer to which we are feeding the normalised vector $\\bar{\\text{x}}$ would then be\n",
    "\n",
    "$$\n",
    "y^{(k)} = \\gamma ^{(k)} \\bar{x}^{(k)} + \\beta ^{(k)}\n",
    "$$\n",
    "\n",
    "These new parameters $\\gamma ^{(k)}, \\beta ^{(k)}$ will be learned by the network during training along with the original parameters.  \n",
    "\n",
    "### PRACTICAL 2.4: Batch Normalisation in `cifar_hyperparam.py`\n",
    "\n",
    "1\\. Use learning decay code from PRACTICAL 2.3\n",
    "\n",
    "2\\. Set the learning rate to 1.00E-03\n",
    "\n",
    "3\\. Set the batch size to 256\n",
    "\n",
    "4\\. Change the log file names to reflect that you are doing *batch normalisation*\n",
    "\n",
    "```python\n",
    "# run_log_dir = os.path.join(FLAGS.log_dir, 'exp_bs_{bs}_lr_{lr}'.format(bs=FLAGS.batch_size, lr=FLAGS.learning_rate))\n",
    "run_log_dir = os.path.join(FLAGS.log_dir, 'exp_BN_bs_{bs}_lr_{lr}'.format(bs=FLAGS.batch_size, lr=FLAGS.learning_rate))\n",
    "```\n",
    "    \n",
    "** Note: You can change the name of your log files for different runs by changing this variable **\n",
    "    \n",
    "5\\. **NOW** to implement Batch Normalisation for the first convolutional layer, define the variable\n",
    "\n",
    "```python\n",
    "   Z1 = conv2d(x_image, W_conv1)\n",
    "```\n",
    "\n",
    "6\\. Use [tf.nn.moments](https://www.tensorflow.org/api_docs/python/tf/nn/moments) to calculate the mean and standard deviation of Z1\n",
    "\n",
    "7\\. Calculate Z1_hat as the batch normalisation of Z1\n",
    "\n",
    "8\\. To learn the new parameters $\\gamma$ and $\\beta$, we just have to create new tensorflow variables:\n",
    "\n",
    "```python\n",
    "    gamma1 = tf.Variable(tf.ones([dimension]))\n",
    "    beta1 = tf.Variable(tf.ones([dimension]))\n",
    "```\n",
    "\n",
    "Tensorflow will cleverly add them to the parameters we want to learn. Of course, you have to work out what `dimension` is! **NOW** replace ```dimension``` with the correct dimension for these variables - one per filter dimension.\n",
    "\n",
    "9\\. The code below illustrates how we can use these variables and the normalised z1_hat.\n",
    "\n",
    "```python\n",
    "    # Scale and shift to obtain the final output of the batch normalisation\n",
    "    # this value is fed into the activation function (here a ReLU)\n",
    "    BN1 = gamma1 * z1_hat + beta1        \n",
    "    h_conv1_bn = tf.nn.relu(BN1) \n",
    "\n",
    "    # Pooling layer - downsamples by 2X.\n",
    "    h_pool1 = max_pool_2x2(h_conv1_bn)\n",
    "```\n",
    "\n",
    "10\\. Let's use it now for our second layer as before.\n",
    "\n",
    "Obviously, there is a function in Tensorflow that helps us a bit! [tf.nn.batch_normalization](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization) but we don't want to use it here\n",
    "\n",
    "Note that the size of gamma2, beta2 differ in this layer\n",
    "\n",
    "11\\. Train your model now with batch normalisation\n",
    "\n",
    "12\\. Save your logs into a new directory ```logs_BN```\n",
    "\n",
    "```\n",
    "Lab_3_<username>.zip\n",
    " |----------logs\\ \n",
    "            |----------exp_BN_bs_256_lr_0.001_train\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    "            |----------exp_BN_bs_256_lr_0.001_validate\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    " |----------run_exp_BN_bs_256_lr_0.001_train,tag_Loss.csv\n",
    " ```\n",
    "\n",
    "\n",
    "After implementing batch normalisation I got **75%** accuracy with learning rate 1.00E-03 and 10000 iterations! That's a 20% boost in accuracy! Quite nice, isn't it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter initialisation (optional)\n",
    "\n",
    "**NOTE: This is an optional part of the lab and is not required for your portfolio**\n",
    "\n",
    "Another decisive element amongst the plethora of factors in training a CNN is parameter initialisation, that is how we assign initial values to the network's weights. What we have done so far was to initialise our parameters taking random values sampled from a [truncated normal distribution](https://en.wikipedia.org/wiki/Truncated_normal_distribution). In our code, we use the following snippet:\n",
    "\n",
    "```python\n",
    "def weight_variable(shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name='weights')\n",
    "```\n",
    "\n",
    "Where the function ```tf.truncated_normal()``` is defined as follows:\n",
    "\n",
    "```python\n",
    "truncated_normal(\n",
    "    shape, # shape of the weight matrix we want to generate\n",
    "    mean=0.0, # mean to generate the distribution\n",
    "    stddev=1.0, # the standard deviation to generate the distribution    \n",
    ")\n",
    "```\n",
    "\n",
    "For our bias parameters, instead, we initialised them to a constant value:\n",
    "\n",
    "```python\n",
    "def bias_variable(shape):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name='biases')\n",
    "```\n",
    "\n",
    "Let's try something silly now, just to realise how important parameter initialisation here.\n",
    "\n",
    "**Q: What happens if we initialise our weights with zeros or ones?**\n",
    "\n",
    "If we run our CNN with these silly initial weights, our accuracy will drop down to 10%, crippling our model and reducing it to a random classifier - a random classifier on 10 classes has a 1/10=10% chance. \n",
    "\n",
    "```python\n",
    "def weight_variable(shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  #initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  initial = tf.zeros(shape) # silly zeros\n",
    "  #initial = tf.ones(shape) # silly ones\n",
    "  return tf.Variable(initial, name='weights')\n",
    "```\n",
    "\n",
    "\n",
    "### Xavier parameter initialisation\n",
    "\n",
    "There are a few strategies to generate initial parameters out there. One of the most used is **Xavier**.\n",
    "\n",
    "This initialiser is designed to keep the scale of the gradients roughly the same in all layers. Weights can be generated using either a uniform or a normal distribution. For the uniform distribution, given a layer $l$ with size $s(l)$, its parameters will be initialised uniformly from the following interval:\n",
    "\n",
    "$$\n",
    "\\Bigg[-\\frac{\\sqrt{6}}{\\sqrt{s(l) + s(l+1)}}, \\frac{\\sqrt{6}}{\\sqrt{s(l) + s(l+1)}} \\Bigg]\n",
    "$$\n",
    "\n",
    "Where $s(l+1)$ is the size of the following layer $l+1$. Note that the size of a layer corresponds in this case to the number of columns of its weight matrix. For the normal distribution, initial weights for a layer $l$ are sampled from a normal distribution whose standard deviation depends on $l$ and $l+i$'s sizes:\n",
    "\n",
    "$$\n",
    "Var[W(l)] = \\frac{2}{s(l) + s(l+1)}\n",
    "$$\n",
    "\n",
    "For a comprehensive explaination of why these magic numbers provide a good parameter initialisation, I recommend you read [the original paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).\n",
    "In tensorflow, using Xavier initialisation is pretty straightforward. All we need to do is to use the initialiser object:\n",
    "\n",
    "```python\n",
    "# the initialiser object implementing Xavier initialisation\n",
    "# we will generate weights from the uniform distribution\n",
    "xavier_initializer = tf.contrib.layers.xavier_initializer(uniform=True)\n",
    "```\n",
    "Then, in our custom functions generating initial weights we simply do:\n",
    "\n",
    "```python\n",
    "def weight_variable(shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  #initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  #initial = tf.zeros(shape)\n",
    "  #initial = tf.ones(shape)\n",
    "      \n",
    "  #return tf.Variable(initial, name='weights')\n",
    "  return tf.Variable(xavier_initializer(shape)) # using Xavier initialisation\n",
    "\n",
    "def bias_variable(shape):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  #initial = tf.constant(0.1, shape=shape)\n",
    "  #return tf.Variable(initial, name='biases')\n",
    "\n",
    "  return tf.Variable(xavier_initializer(shape)) # using Xavier initialisation\n",
    "```\n",
    "\n",
    "Now there is no need to have different functions since both do the same thing.\n",
    "\n",
    "By running our model with Xavier initialisation for 10,000 steps, with batch normalisation and (decaying) learning rate 1.00E-03, we reach 75% accuracy on the test set! Another improvement, yay! \n",
    "\n",
    "**What happens if we use the normal distibution for the Xavier initialisation? Try yourself!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Preparing Lab_3 Portfolio\n",
    "\n",
    "You should by now have the following files, which you can zip under the name `Lab_3_<username>.zip` \n",
    "\n",
    "**From your logs, include only the TensorBoard summaries and remove the checkpoints (model.ckpt-* files)**\n",
    "\n",
    " ```\n",
    " Lab_3_<username>.zip\n",
    " |----------logs\\ \n",
    "            |----------exp_bs_256_lr_0.001_train\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    "            |----------exp_bs_256_lr_0.001_validate\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    "            |----------exp_bs_128_lr_0.0001_train\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    "            |----------exp_bs_128_lr_0.0001_validate\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    "            |----------exp_BN_bs_256_lr_0.001_train\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    "            |----------exp_BN_bs_256_lr_0.001_validate\n",
    "                       |----------events.out.tfevents.xxxxxxxxxx.gpuxx.bc4.acrc.priv\n",
    " |----------run_exp_bs_256_lr_0.001_train,tag_Loss.csv\n",
    " |----------run_exp_bs_128_lr_0.0001_train,tag_Loss.csv\n",
    " |----------run_exp_BN_bs_256_lr_0.001_train,tag_Loss.csv\n",
    " ```\n",
    " \n",
    "Store this zip safely. You will be asked to upload all your labs' portfolio to ** SAFE at Week 9 ** - check SAFE for deadline details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
